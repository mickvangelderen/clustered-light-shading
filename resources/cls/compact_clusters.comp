#include "native/PREFIX_SUM"
#include "native/CLUSTERED_LIGHT_SHADING"
#include "native/PROFILING"

#include "../common.glsl"
#include "cluster_space_buffer.glsl"
// in
#include "cluster_fragment_counts_buffer.glsl"
// use
#include "offsets_buffer.glsl"
// out
#include "cluster_maybe_active_cluster_indices_buffer.glsl"
#include "active_cluster_cluster_indices_buffer.glsl"
#include "draw_commands_buffer.glsl"
#include "compute_commands_buffer.glsl"
#include "profiling_cluster_buffer.glsl"

#if !defined(PASS)
  #error PASS is not defined.
#endif
#if PASS == 0
#define LOCAL_X PASS_0_THREADS
#elif PASS == 1
#define LOCAL_X PASS_1_THREADS
#elif PASS == 2
#define LOCAL_X PASS_0_THREADS
#else
#error Invalid PASS!
#endif

#define tid gl_LocalInvocationID.x
#define wid gl_WorkGroupID.x

shared uint values[LOCAL_X];

layout(local_size_x = LOCAL_X) in;

void sum_2(uint s) {
  uint acc = values[tid] + (tid >= s ? values[tid - s] : 0);
  memoryBarrierShared();
  barrier();

  values[tid] = acc;
  memoryBarrierShared();
  barrier();
}

// Assumes values are loaded in shared memory.
void scan() {
#if (1 << 0) < LOCAL_X
  sum_2(1 << 0);
#endif
#if (1 << 1) < LOCAL_X
  sum_2(1 << 1);
#endif
#if (1 << 2) < LOCAL_X
  sum_2(1 << 2);
#endif
#if (1 << 3) < LOCAL_X
  sum_2(1 << 3);
#endif
#if (1 << 4) < LOCAL_X
  sum_2(1 << 4);
#endif
#if (1 << 5) < LOCAL_X
  sum_2(1 << 5);
#endif
#if (1 << 6) < LOCAL_X
  sum_2(1 << 6);
#endif
#if (1 << 7) < LOCAL_X
  sum_2(1 << 7);
#endif
#if (1 << 8) < LOCAL_X
  sum_2(1 << 8);
#endif
#if (1 << 9) < LOCAL_X
  sum_2(1 << 9);
#endif
#if (1 << 10) < LOCAL_X
#error Loop insufficiently unrolled.
#endif
}

#if PASS == 0
void main() {
  uint blocks_per_dispatch = ceiled_div_u32(cluster_space.cluster_count, PASS_0_THREADS * PASS_1_THREADS);
  uint items_per_dispatch = PASS_0_THREADS * blocks_per_dispatch;

  uint offset_begin = wid * items_per_dispatch;
  uint offset_end = offset_begin + items_per_dispatch;

  // Sum up C*PASS_0_THREADS items in chunks of PASS_0_THREADS.
  uint acc = 0;
  for (uint offset = offset_begin; offset < offset_end;
       offset += PASS_0_THREADS) {
    uint cluster_index = offset + tid;
    if (cluster_index < cluster_space.cluster_count) {
      uint fragment_count = cluster_fragment_counts[offset + tid];
      if (fragment_count > 0) {
        acc += 1;
      }
    }
  }

  // Initialize shared memory
  values[tid] = acc;
  memoryBarrierShared();
  barrier();

  scan();

  // Emit output from shared memory.
  if (tid == 0) {
    offsets[wid] = values[LOCAL_X - 1];
  }
}
#elif PASS == 1
void main() {
  // Initialize shared memory
  values[tid] = offsets[tid];
  memoryBarrierShared();
  barrier();

  scan();

  // Emit output from shared memory.
  offsets[tid] = values[tid];

  // Emit indirect draw and indirect compute data.
  if (tid == LOCAL_X - 1) {
    uint active_cluster_count = values[tid];

    draw_commands.prim_count = active_cluster_count;

    compute_commands[COMPUTE_COMMAND_INDEX_ACTIVE_CLUSTER_COUNT].work_group_x = active_cluster_count;

    uint blocks_per_dispatch = ceiled_div_u32(active_cluster_count, PASS_0_THREADS * PASS_1_THREADS);
    uint items_per_dispatch = PASS_0_THREADS * blocks_per_dispatch;
    uint dispatch_count = items_per_dispatch == 0 ? 0 : ceiled_div_u32(active_cluster_count, items_per_dispatch);

    compute_commands[COMPUTE_COMMAND_INDEX_PREFIX_SUM_LIGHT_COUNTS].work_group_x = dispatch_count;

#if defined(PROFILING_TIME_SENSITIVE)
#if PROFILING_TIME_SENSITIVE == false
    // NOTE: 32*8 should be the layout of light_count_hist.comp.
    compute_commands[COMPUTE_COMMAND_INDEX_ACTIVE_CLUSTER_HIST].work_group_x = ceiled_div_u32(active_cluster_count, 32*8);
    profiling_cluster_buffer.active_cluster_count = active_cluster_count;
#endif
#else
#error PROFILING_TIME_SENSITIVE is not defined.
#endif
  }
}
#elif PASS == 2
void main() {
  uint blocks_per_dispatch = ceiled_div_u32(cluster_space.cluster_count, PASS_0_THREADS * PASS_1_THREADS);
  uint items_per_dispatch = PASS_0_THREADS * blocks_per_dispatch;

  uint offset_begin = wid * items_per_dispatch;
  uint offset_end = offset_begin + items_per_dispatch;

  uint acc = (wid > 0) ? offsets[wid - 1] : 0;
  for (uint offset = offset_begin; offset < offset_end;
       offset += PASS_0_THREADS) {
    uint cluster_index = offset + tid;
    if (cluster_index < cluster_space.cluster_count) {
      // Initialize shared memory
      uint fragment_count = cluster_fragment_counts[cluster_index];
      bool cluster_active = fragment_count > 0;

      values[tid] = cluster_active ? 1 : 0;

      memoryBarrierShared();
      barrier();

      scan();

      uint active_cluster_index_index = acc + values[tid] - 1;

      if (cluster_active) {
        // Instead of writing out the offsets, we use them immediately.
        // We store the link from active_cluster to cluster.
        active_cluster_cluster_indices[active_cluster_index_index] = cluster_index;
      }

      // And we store the link from cluster to active cluster by writing
      // active cluster index + 1, or 0 if the cluster isn't active.
      cluster_maybe_active_cluster_indices[cluster_index] = cluster_active ? active_cluster_index_index + 1 : 0;

      // Add the total sum of the current segment to the accumulator.
      acc += values[LOCAL_X - 1];

      // Ensure shared memory was read in all threads before the next iteration starts writing.
      memoryBarrierShared();
      barrier();
    }
  }
}
#else
#error Invalid PASS!
#endif
